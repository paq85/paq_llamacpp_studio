#!/bin/bash

# llama.cpp Studio - Full Environment Setup Script
# This script sets up the complete environment for GLM-4.7 Flash integration

set -euo pipefail

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Project root directory
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
VENV_DIR="$PROJECT_ROOT/.venv"

# Functions
print_header() {
    echo -e "${BLUE}================================================================${NC}"
    echo -e "${BLUE}  llama.cpp Studio - Environment Setup${NC}"
    echo -e "${BLUE}================================================================${NC}"
    echo ""
}

print_success() {
    echo -e "${GREEN}✓${NC} $1"
}

print_error() {
    echo -e "${RED}✗${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}⚠${NC} $1"
}

print_info() {
    echo -e "${BLUE}ℹ${NC} $1"
}

check_command() {
    if command -v "$1" &> /dev/null; then
        print_success "$1 is installed"
        return 0
    else
        print_error "$1 is not installed"
        return 1
    fi
}

# Print header
print_header

# Check Python
print_info "Checking Python version..."
if check_command python3; then
    PYTHON_VERSION=$(python3 --version 2>&1 | awk '{print $2}')
    print_success "Python $PYTHON_VERSION"
else
    print_error "Python 3 is required. Please install Python 3.9+"
    exit 1
fi

# Check GPU availability
print_info "Checking GPU availability..."
if command -v nvidia-smi &> /dev/null; then
    print_success "NVIDIA GPU detected"
    NVIDIA_AVAILABLE=true
elif command -v rocm-smi &> /dev/null; then
    print_success "AMD GPU detected"
    AMD_AVAILABLE=true
else
    print_warning "No GPU detected (will use CPU if needed)"
    GPU_AVAILABLE=false
fi

# Check required commands
print_info "Checking required commands..."
check_command git || exit 1
check_command curl || print_warning "curl not installed (for model download)"
check_command wget || print_warning "wget not installed (for model download)"

# Setup Python virtual environment
print_info "Setting up Python virtual environment..."
if [ ! -d "$VENV_DIR" ]; then
    print_info "Creating virtual environment in $VENV_DIR..."
    python3 -m venv "$VENV_DIR"
    print_success "Virtual environment created"
else
    print_info "Virtual environment already exists"
fi

# Activate virtual environment
print_info "Activating virtual environment..."
source "$VENV_DIR/bin/activate"
print_success "Virtual environment activated"

# Install Python dependencies
print_info "Installing Python dependencies..."
if [ -f "$PROJECT_ROOT/requirements.txt" ]; then
    pip install -q --upgrade pip
    pip install -q -r "$PROJECT_ROOT/requirements.txt"
    print_success "Python dependencies installed"
else
    print_warning "requirements.txt not found, skipping Python dependencies"
fi

# Setup model directory
print_info "Setting up model directory..."
MODEL_DIR="$HOME/models"
mkdir -p "$MODEL_DIR"
print_success "Model directory created at $MODEL_DIR"

# Setup llama.cpp directory
print_info "Setting up llama.cpp build directory..."
mkdir -p "$PROJECT_ROOT/build/llama.cpp"
print_success "Build directory created"

# Check if llama.cpp is already available
print_info "Checking llama.cpp installation..."
if command -v llama-server &> /dev/null; then
    LLAMA_SERVER_PATH=$(which llama-server)
    print_success "llama-server found at $LLAMA_SERVER_PATH"
    LLAMA_CPP_AVAILABLE=true
else
    print_warning "llama-server not found. Run ./scripts/build_llama.cpp.sh to build it"
    LLAMA_CPP_AVAILABLE=false
fi

# Download GLM-4.7 Flash model if not exists
print_info "Checking for GLM-4.7 Flash model..."
MODEL_FILE="$MODEL_DIR/GLM-4.7-Flash-UD-Q4_K_XL.gguf"
if [ -f "$MODEL_FILE" ]; then
    print_success "GLM-4.7 Flash model found at $MODEL_FILE"
    MODEL_DOWNLOADED=true
else
    print_warning "GLM-4.7 Flash model not found"
    if [ "$GPU_AVAILABLE" = true ] || [ "$AMD_AVAILABLE" = true ]; then
        print_info "GPU detected. Should I download the model now?"
        read -p "Download model (~9GB)? (y/N): " -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            "$PROJECT_ROOT/scripts/download_model.sh"
        fi
    fi
fi

# Create environment configuration
print_info "Creating environment configuration..."
cat > "$PROJECT_ROOT/.env" <<EOF
# llama.cpp Studio Environment Configuration
# Auto-generated by setup.sh

# Python Virtual Environment
VENV_DIR="$VENV_DIR"

# Model Path
MODEL_PATH="$MODEL_FILE"

# Server Configuration
SERVER_HOST=127.0.0.1
SERVER_PORT=11433

# GPU Configuration
GPU_TYPE=$([ "$NVIDIA_AVAILABLE" = true ] && echo "nvidia" || echo "amd")
MIN_VRAM_MB=8192

# Build Configuration
LLAMA_CPP_DIR="$PROJECT_ROOT/build/llama.cpp"
EOF
print_success "Environment configuration created"

# Create startup script
print_info "Creating startup script..."
cat > "$PROJECT_ROOT/scripts/start_server.sh" <<'SCRIPT'
#!/bin/bash

# llama.cpp Studio - Server Startup Script
# Start llama-server with GLM-4.7 Flash optimizations

set -euo pipefail

# Source environment configuration
ENV_FILE="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)/.env"
if [ -f "$ENV_FILE" ]; then
    source "$ENV_FILE"
fi

PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
VENV_DIR="${VENV_DIR:-$PROJECT_ROOT/.venv}"
MODEL_PATH="${MODEL_PATH:-$HOME/models/GLM-4.7-Flash-UD-Q4_K_XL.gguf}"

# Colors
GREEN='\033[0;32m'
BLUE='\033[0;34m'
NC='\033[0m'

echo -e "${BLUE}Starting llama.cpp Server with GLM-4.7 Flash${NC}"
echo "Model: $MODEL_PATH"

# Activate virtual environment if needed
if [ -f "$VENV_DIR/bin/activate" ]; then
    source "$VENV_DIR/bin/activate"
fi

# Start llama-server
python3 -m tools.llama_bench.cli server \
    --model "$MODEL_PATH" \
    --monitor \
    --allow-multi-server \
    --ctx-size 200000 \
    --batch-size 4096 \
    --ubatch-size 1024 \
    --flash-attn on \
    --kv-unified on \
    --enable-thinking true \
    --host "${SERVER_HOST:-127.0.0.1}" \
    --port "${SERVER_PORT:-11433}"
SCRIPT
chmod +x "$PROJECT_ROOT/scripts/start_server.sh"
print_success "Startup script created"

# Create stop script
print_info "Creating stop script..."
cat > "$PROJECT_ROOT/scripts/stop_server.sh" <<'SCRIPT'
#!/bin/bash

# llama.cpp Studio - Server Stop Script
# Stop all llama-server processes gracefully

set -euo pipefail

echo "Stopping llama.cpp Server(s)..."

# Stop via llama-run
if [ -f "$PROJECT_ROOT/llama-run" ]; then
    "$PROJECT_ROOT/llama-run" stop-all
fi

# Stop direct processes
pkill -f "llama-server" || true
pkill -f "llama-bench.*server" || true

# Kill processes on port 11433
lsof -ti:11433 | xargs -r kill -9 || true

echo "Server(s) stopped"
SCRIPT
chmod +x "$PROJECT_ROOT/scripts/stop_server.sh"
print_success "Stop script created"

# Final summary
echo ""
echo -e "${GREEN}================================================================${NC}"
echo -e "${GREEN}  Setup Complete!${NC}"
echo -e "${GREEN}================================================================${NC}"
echo ""
echo -e "Next steps:"
echo -e "  1. Start the server: ${BLUE}./scripts/start_server.sh${NC}"
echo -e "  2. Run benchmarks: ${BLUE}./scripts/test_glm.sh${NC}"
echo -e "  3. Check documentation: ${BLUE}./docs/GLM-4.7_SETUP.md${NC}"
echo ""
echo -e "Server will run at: ${BLUE}http://127.0.0.1:${SERVER_PORT:-11433}${NC}"
echo ""
